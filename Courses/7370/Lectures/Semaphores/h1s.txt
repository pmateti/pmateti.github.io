
CS 7370 Distributed Systems I  (Spring 1991: Mateti)

Home Work #1 Solutions
Sun May  5 16:15:55 1991


Exercise 1: Critically study and compare the algorithms for starvation free
solutions to the mutex problem using weak semaphores by Udding[86] and
Morris[79].

(My answer is rather long because the algorithms generated much discussion.)

The basic idea is the same in both: Imagine the critical section <cs> to be
enclosed in two nested areas, each with its own gate.  Certain number of
processes are let in through the outer gate, and it is then closed.  Passing
through the inner gate is the granting of exclusive access to the cs.  Until
all these processes are through with the cs, the outer gate is not opened
again.  Thus, there is no starvation for any process that has managed to enter
through the outer gate.  We must, however, be careful in not starving a process
at the outer gate.

First, let me summarize the differences you can note by studying the code of
the two solutions.  For that kind of comparison, it is important that we
reproduce the two solutions while getting rid off superficial differences.  We
rename the variable na of Morris to be ne, and rename all the semaphores to
have a `u' as a suffix for Udding's solution, and `m' as a suffix for Morris'.
We also rearrange the if-prongs of Morris' solution.

Udding's solution		| Morris' solution

 1 P(eu); ne := ne+1; V(eu);	| P(em); ne := ne+1; V(em);
 2 P(qu); P(eu);		| P(qm);
 3	nm := nm+1;		|	nm := nm+1;             
 4	ne := ne-1;		|	P(em); ne := ne-1;
 5	if ne > 0 --> V(eu)	|	if ne > 0 --> V(em);V(qm)
 6	[] ne = 0 --> V(mu)	|	[] ne = 0 --> V(em);V(mm)
 7	fi;			|	fi;
 8 V(qu);			|
 9 P(mu); nm := nm-1;		| P(mm); nm := nm-1;             
10	<cs >			|	<cs >                   
11	if nm > 0 -->  V(mu)	|	if nm > 0 -->  V(mm) 
12	[] nm = 0 -->  V(eu)	|	[] nm = 0 -->  V(qm) 
13	fi			|	fi


The split binary semaphores su and sm are: su = eu+mu, and sm = qm+mm.
Variable ne is protected by em or eu, and nm is protected by sm or su.  The
roles of mu and mm are nearly the same; they are the inner gates.  The outer
gates are eu and qm, as can be seen from lines 11--13.  But, the second P(eu)
is preceded by a P(qu).  The role that the mutex semaphore qu plays with
repsect to eu/su is different from that of the mutex semaphore em with respect
to sm.  P(qu) guarantees that at most one process is waiting on the P(eu) of
line 2.  The em protects ne and that is its *sole* purpose in Morris (thus, we
can move P(em) from line 4 to line 2 and place it after P(qm) which makes the
two algorithms even more similar).  The eu does double duty, however --- it
protects ne, as well as being the outer gate.

Second, let us examine how the authors define weak semaphores.  Strong and weak
semaphores behave identically if there are no waiting processes when a V(s) is
done.  So, let us assume that, at time t0, the set of waiting processes W(s,
t0) is nonempty when (i.e., at time t0) a V(s) is done by process Y.  Clearly,
Y is not in W(s, t0).  Each process p in W(s, t0) has initiated a P(s), but has
not completed.  Consider *the* P(s) operation that was completed at time t1 >=
t0, where t1 is such that during t0..t1 no P(s) was completed; i.e., this P(s)
at t1 was the first to get finished after time t0.  Two questions regarding
which we must analyze the authors' assumptions are: (Q1) Which process X was
able to finish P(s) at t1? (Q2) Is t1 = t0?  Note that if t1 > t0, during
t0..t1, additional processes may have begun P(s); i.e., for t0 <= t <= t1, W(s,
t) = W(s, t0) plus possibly a few more.

Udding's assumption(s) appear scattered.  "A process that executes a
V-operation on a semaphore will not be the one to perform the next P-operation
on that semaphore, if a process has been blocked at that semaphore.  One of the
waiting processes is allowed to pass the semaphore." (p159, left column) I
understand this as saying that X is in W(s, t0).  "... one of the processes
blocked at a semaphore will pass that semaphore when a corresponding
V-operation is performed."  (p161, left column) This reaffirms that X is in
W(s, t0) and goes further that t1 = t0.

Morris thankfully states his assumption in one place as Property 1.1: "If any
process is waiting to complete a P(s) operation, and another process performs a
V(s), the semaphore is not incremented and one of the waiting processes
completes its P(s)."  Obviously, the author means to add 'without decrementing
s' in the phrase "completes its P(s)."  With this proviso, X is in W(s, t0) and
t1 = t0.  Note that it is em that must have the property 1.1; qm and mm can be
even weaker.

Thus, the two authors are making `equivalent' assumptions.  It is an
interesting and publishable exercise to prove this equivalence rigorously.

Third, let us give at least one fact to justify the statement by Udding (p159,
left column) that "Although it [his algorithm] closely resembles the solution
of Morris, its operation is quite different."  because Udding does not.
Suppose V(mu) of line 6 was executed.  Clearly, ne = 0 and nm > 0.  In Udding,
once the outer gate is closed (eu = 0), ne will remain a 0 until nm becomes 0.
In Morris, after line 6 is executed, ne can grow to be positive even though nm
is still > 0.

It is worth recalling that Udding does not claim his algorithm to be superior
(in elegance or efficiency) compared to Morris; his claim is about the clarity
of derivation of the algorithm.

----------------------------------------------------------------------
Exercise 3: Reverse the order of the first two statements of G.
Peterson's mutual exclusion aglorithm.  Is this safe to do?

The problem is that mutual exclusion is not provided any more.  The
following sequence of execution is possible letting both P1 and P2 to
be in their <cs>.

initially, flag1 = flag2 = false;

P1				P2

turn := 2;
				turn := 1; 
				flag2 := true;
				while flag1 and turn=1 do od;
				<cs>
flag1 := true;         		:
while flag2 and turn=2 do od;	:
<cs>;				:

----------------------------------------------------------------------
Exercise 5: A vector multiplied with a matrix in CSP.

The code in Section 6.2 of Hoare's paper solves this problem.  You need only
store the matrix in the center, input the vector from west and 0s from the
north.  Then, the resultant vector will come from the south.

---------------------------------------------------------------------- 

Exercise 4.  Solve the exercise in Section 4.6 of Hoare's CSP paper.  That is,
provide a least() "remove the least member" operation in the small set of
integers process-collection.  The fat-bar is shown as '|'

S(i: 1..100) ::
n: integer; empty: boolean; empty := true;
*[ empty -->
	[ S(i-1)?has(n) --> S(0)!false
	| S(i-1)?insert(n) --> empty := false
	| S(i-1)?least() --> S(i-1)!nonleft()
	]
   not empty -->
	[m:integer; S(i-1)?has(m) -->
		[ m <= n --> S(0)!(m = n)
		| m >  n --> S(i+1)!has(m)
		]
	|m:integer; S(i-1)?insert(m) -->
		[ m < n --> S(i+1)!insert(n); n := m
		| m = n --> skip
		| m > n --> S(i+1)!insert(m)
		]
	|S(i-1)?least() -->
		S(i-1)!n;
		S(i+1)!least(); 
		[ S(i+1)?n --> skip;    
		| S(i+1)?nonleft() --> empty := true;
		]
	]
 ]

----------------------------------------------------------------------
Exercise 2. Generalize Peterson's algorithm with efficiency concern

