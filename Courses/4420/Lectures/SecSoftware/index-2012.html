<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html>
<head>
  <title>Software without Security Holes by Mateti</title>
  <meta name="author" content="Prabhaker Mateti">
  <meta name="keywords" content=
  "Robust Programming, Internet security, Network security, TCP/IP security">
  <meta name="description" content=
  "This page is part of a set of lectures notes for a course on Internet Security by Prabhaker Mateti http://www.cs.wright.edu/~pmateti/">
  <meta http-equiv="Content-Style-Type" content="text/css">

<base target="_top">

<style type="text/css">
  a:hover      { color: blue; background-color: yellow; }
  a:active     { color: white; background-color: green;}
  P { text-align: justify }
  pre {border:blue 1px dotted;}
  tt {border:blue 1px dotted;}
  ol.o {padding-left: 0px; }
  li { text-align: justify; }
  @page { size: 8.5in 11in }
  @media screen {BODY {white; margin: 15%} }
</style>
</head>

<body>


  <h1>Software without Security Holes</h1>

  <h2><a target="_blank" href=
  "http://www.cs.wright.edu/~pmateti">Prabhaker Mateti</a></h2>

  <p align="justify">Abstract:  This lecture is about
  developing good habits and learning techniques that prevent
  errors in security software.</p>

  <p><a href="SoftwareNoSecHoles.pptx">SoftwareNoSecHoles.pptx</a></p>

  <h2>Table of Contents</h2>

  <ol>
    <li><a href="#Educational%20Objective">Educational
    Objectives</a></li>

    <li>
      Software
      without Security Holes

      </li>

    <li>Top Ten
        Security Holes</li>
	<li>Robust Programs, Correct Programs and Secure
        Programs</li>
	<li>Coding Practices</li>
	<li><a href="#Writing%20Safe%20setuid%20Programs">Writing Safe
    setuid Programs</a></li>

    <li><a href=
    "#Correct%20By%20Design%20and%20Mathematical%20Proof">Correct
    By Design and Mathematical Proof</a></li>

    <li><a href="#LabExperiment1">Lab Experiment-1</a></li>
    <li><a href="#LabExperiment2">Lab Experiment-2</a></li>

    <li>Acknowledgements</li>

    <li><a href="#References">References</a></li>
  </ol>

  <h2><a name="Educational Objective">
      Educational Objectives</a></h2>

  <ol>
    <li>Understand the contribution to insecurity made by
    programming errors.</li>

    <li>Be able to deploy robust programming techniques.</li>
  </ol>

  <h1>Software without Security Holes</h1>

  <p>"All problems fall into one of two categories: Those that can
  and those that cannot be easily solved. For instance, some of the
  denial of service attacks ... are a result of the IP protocol's
  design. Short of implementing a new protocol ..., not much can be
  done beyond stopgap measures that make particular attacks less
  effective.</p>

  <p>Other difficult problems include network sniffing and
  spoofing. These result from security-related information being
  sent in the clear over networks. Then there is the general
  authentication problem. The difficulty with authentication is
  that the lowest common denominator is user names and passwords,
  and that method is generally not sufficient.</p>

  <p>Unfortunately, solving these problems requires new hardware,
  new software, and user training, ... Over the longer term,
  protocols like IPV6 and IPsec will resolve many of these issues.
  Of course they may create new ones. ...</p>

  <p>The solvable problems are the result of poor planning,
  programming, and implementation. These can be solved by software
  vendors ... to improve their coding methodologies. ... "</p>

  <p>"... Is code getting better? You could assume that the
  security holes in operating systems are the result of poor coding
  way back when, and that new code and coding methods do not have
  the same problem. You would be wrong. Consider Windows NT and its
  sorry security state. Or look in our own back yard at Solaris.
  Bugs in <tt>admintool</tt>, NIS+, the volume manager,
  <tt>procfs</tt>, PPP, PAM, and the PCI bus drivers ... prove
  the point." [Peter Galvin, Pete's Wicked World column, Sun World
  Magazine, 1998]</p>

  <h2><a name="Top Ten Security Holes">Top Ten Security
  Holes</a></h2>

  <p>In 2000, GSA Federal Chief Information Officers Council
  listed the "The Ten Most Critical Internet Security Threats":</p>

  <ol>
    <li>BIND weaknesses: nxt, qinv and in.named allow immediate
    root compromise.</li>

    <li>Vulnerable CGI programs and application extensions (e.g.,
    ColdFusion) installed on web servers.</li>

    <li>Remote Procedure Call (RPC) weaknesses in rpc.ttdbserverd
    (ToolTalk), rpc.cmsd (Calendar Manager), and rpc.statd that
    allow immediate root compromise</li>

    <li>Remote Data Services (RDS) security hole in the Microsoft's
    web server named IIS.</li>

    <li>Sendmail buffer overflow weaknesses, pipe attacks and
    MIMEbo, that allow immediate root compromise.</li>

    <li>Buffer overflows in sadmind (remote administration access
    to Solaris systems) and mountd (controls and arbitrates access
    to NFS mounts on UNIX hosts) permit root compromise.</li>

    <li>Global file sharing and inappropriate information sharing
    via NFS and Windows NT ports 135-&gt;139 (445 in Windows2000)
    or UNIX NFS exports on port 2049. Also Appletalk over IP with
    Macintosh file sharing enabled.</li>

    <li>User IDs, especially root/administrator with no passwords
    or weak passwords.</li>

    <li>IMAP and POP buffer overflow vulnerabilities or incorrect
    configuration.</li>

    <li>Default SNMP community strings set to &#8216;public&#8217;
    and &#8216;private.&#8217;</li>
  </ol>

  <p>Observe how many of these are due to programming errors.  Items
  1-6, and 9 are programming errors.  Items 7, 8, and 10 are
  configuration errors.  Note that none of the top ten are due to
  design errors in TCP/IP.</p>

  <h2>Robust, Correct and Secure Programs</h2>

  <p>For the purpose of this article, let us define robustness as
  being crash proof, and hang-proof no matter what the inputs
  are.  Crash is unexpected termination.  A hang is
  unexpected non-termination.  Two classes of being hung are:
  infinite looping, and waiting for an event that will not
  occur.  Infinite looping consumes heavily the CPU
  time.  Waiting for a non-occurring event consumes almost no
  resources.  Note that infinite recursion will lead to a
  crash via resource exhaustion.</p>

  <p>When you're writing a normal piece of software, your purpose is
  to make certain things possible, if the user does things correctly.
  When you're writing a security-sensitive piece of software, you also
  must make certain things <em>impossible</em>, no matter what any
  trusted or untrusted user does. Cryptologists and real-time
  programmers are familiar with doing things this way. Most other
  programmers are not.</p>

  <p>The following programs must be robust:  The OS
  kernel.  All <tt>setuid</tt> and <tt>setgid</tt>
  programs. All daemons that accept network connections.</p>

  <p>If the kernel has security holes, no amount of checking of system
  programs is going to make the system secure from attack.  However,
  relatively few kernel bugs are being found and exploited these days.
  In terms of security, kernels are relatively bug-free because of the
  limited interfaces available to attack. For instance, Solaris has
  210+ system calls (check <tt>/usr/include/sys/syscall.h</tt>), and
  Linux kernel of a few years ago about 190 and in 2012 about 320+.
  Compare that to the thousands of points a hacker has available to
  attack: sockets, files, devices, and programs.</p>

  <p>Is it possible to develop secure programs?  Yes, but the cost of
  developing such software is so high that neither customers not
  companies are willing to spend.  It is also the case, that placing
  high importance on security will lead to these programs being
  inefficient.  In the next few paragraphs of this section, we ignore
  both these issues (cost and efficiency) and speculate on the
  possibility of developing secure software.</p>


  <h2>Coding Practices</h2>

  <p>All large programs are buggy.  It is unfortunate, but this axiom
  captures the state of the technology.  Security-relevant programs
  have security bugs. Large programs are buggier than their size would
  indicate.  Good coding practices can make large programs less buggy.

  <p>Buffer Overflow.  Be sure all buffers (arrays of
    items, usually characters) are bounded.  Do bounds
    checking on every variable before the contents are copied to a
    local buffer.  Avoid routines that fail to check buffer
    boundaries when manipulating strings, particularly:
    <tt>sprintf(), fscanf(), scanf(), vsprintf(), realpath(),
    getopt(), getpass(), streadd(), strecpy(), strtrns(),
    gets()</tt>, <tt>strcpy()</tt>, and
    <tt>strcat()</tt></p>

  <p>Files and Directories. Always use full pathnames for any
    file arguments.  The current directory assumed by your
    program may not be where it is at.  Explicitly change
    directories (<tt>chdir()</tt>) to an appropriate directory
    at program start. If creating a new file, use <em>O_EXCL</em>
    and <em>O_CREAT</em> flags to assure that the file does not
    already exist. Do not create files in world-writable
    directories. Use <tt>lstat()</tt> to make sure a file is
    not a link, if appropriate. Set limit values to disable
    creation of a core file if the program fails. If using
    temporary files, consider using <tt>tmpfile()</tt> or
    <tt>mktemp()</tt> system calls to create them (although
    most <tt>mktemp()</tt> library calls have problematic race
    conditions).</p>

    <p>Be aware of race conditions, deadlock conditions and
    sequencing conditions.</p>

    <p>Never use <tt>system()</tt> and <tt>popen()</tt>
    system calls</p>

    <p>Do not make assumptions about port numbers, use
    <tt>getservbyname()</tt> instead.  Do not assume
    connections from low-numbered ports are legitimate or
    trustworthy. Do not assume the source IP address is legitimate.
    Place timeouts and load level limits on incoming
    network-oriented read request.  Place timeouts on outgoing
    network-oriented write requests.</p>

  <p>Daemons allow users to access the system without first
    getting authenticated.  A network daemon may answer a network
    request and process it under the daemon's privileges, not a
    user's. Therefore, this is another way for users to increase
    access, or even gain initial access, to the target system.</p>


    <p>Robust Compilation and Libraries. Make good use of tools such
    as <tt>lint</tt> and <tt>splint</tt>.  Have internal
    consistency-checking code.  Use your compiler wisely. With gcc,
    use <tt>-Wall -ansi -pedantic</tt> flags.  Use safe libraries.</p>

    <p>Have code reviewed by other people. E.g., commercial
    products such as 3Com's  CoreBuilder and SuperStack II
    hubs were revealed to have "secret" backdoor passwords.</p>

    <p>Test thoroughly.  Test the software using the same
    methods that crackers do: Try to overflow every buffer in the
    package, Try to abuse command line options, Try to create every
    race condition conceivable.  Have others besides the
    designers and implementers test the code.  Be aware of
    test coverage; gcc -pg -a causes the program to produce a
    bb.out file that is helpful in determining how effective your
    tests are at covering all branches of the code. </p>

  <h2><a name="Writing Safe setuid Programs">Writing Safe setuid
  Programs</a></h2>

    <p>Avoid creating <tt>setuid</tt> or <tt>setgid</tt> shell
    scripts.  The setuid feature allows executables launched by an
    ordinary user to run with root privileges. A typical example is
    the passwd program. Attackers exploit setuid programs in order to
    gain root level access. Therefore, a system administrator should
    hunt down all the setuid programs on a system. and remove the
    setuid bit, or very thorughly evaluate why it must remain set.</p>

  <p>Read the man page on setuid carefully.  Like most man pages, the
    descriptions of this most famous syscall vary from Unix to Unix.
    Unfortunately, the man pages are quite unclear, and many
    programmers do not study other carefully written setuid
    programs. </p>

  <p>Check for "rws----" permissions to see if an executable is
    setuid root. Run <tt>find / -perm +4000 -print</tt> to locate
    all setuid files. Add "-user root" in order to find just those
    that elevate to root.</p>

  <h2>Economy of Mechanism</h2>

  <p>Exposed machines should run as few programs as possible;
  the ones that are run should be as small as possible.

  <p>Keep your implementation as simple as possible. Note
  that <em>simple</em> is different from
  <em>small</em>: just because you <em>can</em> write a CGI program
  in 300 bytes of line-noise Perl, doesn't mean you
  <em>should. </em> All the usual structured-programming tips
  help here: clean interfaces between modules, avoid global state,
  etc.

  <p>Keep Interactions Minimal. You often need to check how each
  pair of subsystems interacts, and possibly even each
  <em>subset</em> of subsystems.  For example, interactions
  between the password checker and the page-fault mechanism.</p>

  <p>Least Common Mechanisms. The assumptions originally made in
  shared code may no longer be valid.  Eaxmple 1: Netscape's
  LiveConnect allows Java and Javascript and the browser to talk to
  each other.  But Java and Javascript have different ways to
  get at the same information, and also different security
  policies.  A malicious Java applet could cooperate with a
  malicious Javascript page to communicate information neither
  could have communicated alone. Example 2. Windows exports an easy
  interface to IE's HTML-rendering code. Mail clients like Eudora,
  Outlook Express, among other programs, use this interface to
  display HTML-formatted email.  By default, parsing of Java
  and Javascript (J-Script) are enabled.  However, the
  HTML-rendering code "thinks" that Java and J-Script are unsafe
  when loaded from the Internet, but safe when loaded from local
  disk.   The email is loaded from local disk!</p>

  <p>Make the program's critical portion as short and simple as
    possible.</p>



  <h2>Fail-Open or Fail-Closed?</h2>

  <p>Security can fail in two different
  ways: Allow access when it shouldn't; this is called
  fail-open.  Refuse access when it shouldn't; this is called
  fail-closed.  As an example, consider electronic door
  lock.  When the power goes out, locking the door by holding
  it closed with a massive electromagnet will fail-open, whereas
  locking the door with a spring-loaded deadbolt that is pulled out
  of the way with a solenoid will fail-closed.

  <p>Many programs do not check if enough resources will be
  available. What happens if there is not enough memory and some
  allocations fail? What happens if the program runs out of fille
  descriptors? What happens if the program cannot fork()?</p>

  <h2>Security compartments</h2>A secure system is divided into
  security compartments. For example, a Linux system has numerous
  compartments known as "users", "kernel", and  "network",
  which is divided into sub compartments known as "network
  connections". There are well-defined trust relationships between
  these different compartments, which are based on system setup and
  authentication.  The trust relationships must be enforced at
  every interface between security compartments.

  <h2>Trusting Untrustworthy Channels</h2>

  <p>If you send passwords in
  clear text over a LAN, if you create a world-writeable file and
  later try to read back data from that file, if you create a file
  in /tmp with O_TRUNC but not O_EXCL, etc., you are trusting an
  untrustworthy intermediary.</p>

  <p>Do not assume that inputs are valid.  E.g., if an
    argument should be a positive integer in the range of 2 to 7,
    verify that.  If an argument should be a non-empty string
    of letters not exceeding 13 characters in length, verify
    that.  Check interactive input to be sure it contains only
    "good" characters.  Consider how such input will be parsed
    when substituted.  Check arguments passed in environment
    variables.</p>

  <p>Do not require clear-text authentication information.</p>

  <p>Use session encryption to avoid session hijacking and hide
    authentication information.</p>



  <h2>Proper defaults</h2>

  <p>If there are non-obvious, but insecure,
  defaults, it is likely that system administrators will leave them
  alone. For example, if you unpack an RPM or a ZIP archive and it
  creates some configuration files world-writeable, you are
  unlikely to notice.

  <h2>Error Handling and Reporting</h2>

  <p>Error handling and reporting is an essential part of any
  programming paradigm. Delicate handling of and recovery from
  error conditions is an absolute necessity, especially in a third
  party library.</p>

  <p>Check all system call parameters and system call return
    code.   System calls should verify their arguments,
    but unfortunately most OS calls do not for fear of becoming
    inefficient, so you must.  Fortunately, all system calls
    return a success or failure code.  Unfortunately, only a
    few programs verify these result codes.</p>

  <p>Logging Events.  Do log relevant information,
    including date, time, uid and effective uid, gid and effective
    gid, terminal information, pid, command-line arguments, errors,
    and originating host.  Make sure that the log files
    themselves remain bounded in size.</p>


  <h2>Change-of-role hole</h2>

  <p>What was originally a minor annoyance, or sometimes even a
  convenience, can become a security hole when a program is run in
  a different context.  For example, suppose you have a
  PostScript interpreter that was originally intended to let you
  preview your documents before printing them. This is not a
  security-sensitive role; the PostScript interpreter doesn't have
  any capabilities that you do not. But suppose you start using it
  to view documents from other people. Suddenly, the presence of
  PostScript's file access operators becomes a threat! Someone can
  send you a document which will delete all your files -- or
  possibly stash copies of your files someplace they can get at
  them.</p>

  <h2><a name="Correct By Design and Mathematical Proof">Correct By
  Design and Mathematical Proof</a></h2>

  <p>There is a large body of technical literature that advocates
  designing software by first writing formal specifications capturing
  the all requirements of the software yet to be constructed.
  Subsequent steps are systematic refinements of such specifications
  yielding several levels of designs ultimately producing the source
  code in a programming language.  The writings of Turing-award
  winners Dijkstra and Hoare are heavily influential in this
  regard. (For an elementary introduction, read "Practical Advice on
  Writing Pre- Post-Conditions for Real Programs," listed in the
  References.) Unfortunately, the open literature has documented only
  small programs that are so developed.  Most of the academic computer
  science community as well as the industry believes that such
  development is astronomically expensive and even then not
  necessarily qualitatively "better."</p>

  <a name="SPLINT"></a>
  <p>In recent years, more practical uses of the above methodology
  have emerged.  E.g., a tool, called SPLINT (see References), can
  analyze large amounts of C source code at a speed comparable to that
  of a typical compiler and flag a variety of notorious errors that
  made secure software succumb to such attacks as buffer overflow.</p>

  <p>Use formal specifications. At a minimum, develop pre- and
    post-conditions in carefully written English.</p>

  <p>assert(3) is a macro that accepts a single argument which it
  treats as a Boolean expression. If the expression evaluates to
  false, the assert macro prints an error message and terminates
  the program. Assertions are useful in the developmental stages of
  programs when verbose error handling is not in place or when a
  grievous error condition that normally should not happen
  occurs.  One must use assertions, but exiting abruptly even
  after reporting an error is not acceptable. If a grievous error
  condition is detected, the code should return error codes to the
  caller, and let it decide what to do. Code should be able to
  handle grievous errors well enough to be able to exit gracefully
  from the top level (if possible).  Also, never use
  structured exception handling as a substitute for writing solid
  code in the first place.</p>

  <a name="LabExperiment1"></a>
  <h2>Lab Experiment-1</a></h2>

  <p>This lab, and the next lab-2, are both <i>exempt</i> from the
  usual rule: {All work should be carried out in Operating Systems and
  Internet Security (OSIS) Lab, 429 Russ.  Use any of the PCs numbered
  23 to 30.  No other WSU facilities are allowed.} </p>

  <p>Objective: Introduce you to formal methods based tools.  Get you
    to think about secure programming some more.  </p>

  <ol>
    <li>Study Matt Bishop's article.  Provide answers to Exercises 10
    and 19 in the lab report.</li>

    <li>List (as in <tt>ls -l</tt>) all the setuid programs in
      Knoppix.</li>

    <li>Run <tt><a href="#SPLINT">splint</a></tt>
      on <tt>exploit4.c</tt> of Aleph One.  Revise the code
      of <tt>exploit4.c</tt>, and adjust the flags of <tt>splint</tt>
      so that all errors and warnings shown by <tt>splint</tt> are
      gone.  Include in the lab report the content of
      exploit4Revised.c</li>

    <li>Download the source code of both the locally stored version of
      2003 <a href= "sudo-1.6.7p5.tar.gz">sudo-1.6.7p5.tar.gz</a>, and
      the latest from <a href="http://www.sudo.ws/sudo/">
      http://www.sudo.ws/sudo/</a>.  Build one of the two as usual.
      Check that it "works."</li>

    <li>Run <tt>splint</tt>, with no flags (except for
      include-related), collectively on all the source code files of
      sudo.  Insert all its messages into the lab report.</li>

    <li>Select three interesting messages regarding source code errors
      generated by splint, and explain the messages and the causes for
      their generation.</li>

    <li>Bonus Points: The tar ball sizes of the two sudo versions you
    downloaded are shown below.  Summarize what has improved.  Explain
    the increase in size.
<pre>
-rw-r--r-- 1 pmateti pmateti  349785 2003 sudo-1.6.7p5.tar.gz
-rw-r--r-- 1 pmateti pmateti 1608969 2012 sudo-1.8.4p4.tar.gz
</pre>

    <li>Bonus Points: Study the different versions of the
      <a href="./sudo-man-pages/"> man pages of sudo</a>.  Focus on
      the "seven sins of the specifier", namely, 1. Noise, 2. Silence:
      3. Overspecification, 4. Contradiction, 5. Ambiguity, 6. Forward
      Reference, 7. Wishful Thinking [from Bertrand Meyer,
      <a href="http://se.inf.ethz.ch/old/people/meyer/publications/computer/formalism.html">
      On Formalism In Specifications </a>, IEEE Software, 1985,
      vol. 2, no. 1. pp. 6-26]. Write up your findings.
  </ol>

 <p><a href="SecSoftwareLabGS.html">Lab-1 Grading Sheet</a></p>

  <a name="LabExperiment2"></a>
  <h2>Lab Experiment-2</a></h2>

  <p>Objective: Introduce you to a few more static and dynamic
    analysis tools that help discover security bugs.  Three such tools
    are briefly described below.</p>

    <p>[Rough Auditing Tool for Security] RATS is a security auditing
      utility for C, C++, PHP, Perl, and Python source code. RATS
      scans source code, finding potentially dangerous function calls.
      The goal of rats is to provide a starting point for performing
      manual security audits.  See the book "Building Secure Software"
      by Viega and McGraw, 2001.}  RATS is a standard package in
      Debian: <tt>apt-get install rats</tt>.  Read <tt>man rats</tt>.
    </p>

    <p>[<a href="http://valgrind.org/">http://valgrind.org/</a>] A
    fairly simple use of valgrind is to monitor a process to detect
    coding errors such as uninitialized variables, index overflow of
    an array, or memory leaks.
      <tt>apt-get install valgrind kcachegrind</tt> as these are
      standard packages in Debian; read <tt>man valgrind
      kcachegrind</tt>
    </p>

    <p>
      [UNO 2007] Uno is not a standard Debian package;  build it from
    the source code downloadable from
      <a href="http://spinroot.com/uno/">
    http://spinroot.com/uno/</a>.  It is invoked as in <tt>uno
    file.c</tt>.  It intercepts (i) Use of uninitialized variables,
    (ii) Null-pointer references, and (iii) Out-of-bounds array
    indexing.  Other properties to be checked can be specfied in a
    C-like language.  For instance: checking lock order disciplines,
    compliance with user-defined interrupt masking rules, rules
    stipulating that all memory allocated must be freed, etc.  There
    are examples in the uno/prop/ directory.
  </p>


  <ol>
    <li>
      Continue to work with the source code files of the latest sudo
      from <a href="http://www.sudo.ws/sudo/">
      http://www.sudo.ws/sudo/</a>.  Build it as usual.  Check that it
      "works."</li>
    <li>Use each of the above tools on the source code files and/or the
    sudo program built from the above.</li>
    <li>Select and discuss five discoveries made by each of the tools.
    Total 15 items.</li>
    <li>Write a comparative review of the tools.
    <li>Holzmann's paper (see the References) presents "Ten Rules for
    Writing Safety Critical Code":
      <ol type=i>
<li> Restrict to simple control flow constructs.</li>
<li> Give all loops a fixed upper-bound.</li>
<li> Do not use dynamic memory allocation after initialization.</li>
<li> Limit functions to no more than 60 lines of text.</li>
<li> Use minimally two assertions per function on average.</li>
<li> Declare data objects at the smallest possible level of scope.</li>
<li> Check the return value of non-void functions, and check the validity of function parameters.</li>
<li> Limit the use of the preprocessor to file inclusion and simple macros.</li>
<li> Limit the use of pointers. Use no more than two levels of dereferencing per expression.</li>
<li> Compile with all warnings enabled, and use one or more source code analyzers.
  </ol>
      Comment on whether sudo follows these rules.


  </ol>


 <p><a href="SecSoftwareLab2GS.html">Lab-2 Grading Sheet</a></p>

  <h2><a name="Acknowledgements">Acknowledgements</a></h2>

  <p>These lecture materials are gleaned from many sources.  All are
  presented after careful reading.  In some cases, I may have
  unintentionally neglected proper attribution. I assure the reader it
  is not because I claim authorship.  Indeed, in the lectures there is
  hardly any thing new that I have contributed.  Suggestions for
  improvement are always welcome. </p>


  <h2><a name="References">References</a></h2>

  <ol>
    <li><a href="http://nob.cs.ucdavis.edu/%7Ebishop"> Matt
	Bishop</a>, (i) Robust Programming, 
	[<a href="http://nob.cs.ucdavis.edu/%7Ebishop/secprog/robust.pdf">
	PDF</a>]  October 1998.  Required Reading.
      (ii) Secure setuid programs.
      <a href="./setuidBishop1996-sans-tut.pdf">
	Local Copy</a>  Required Reading.</li>

    <li>David Evans and David Larochelle, Improving Security Using
      Extensible Lightweight Static Analysis,
      <a href="http://www.cs.virginia.edu/~evans/pubs/ieeesoftware-abstract.html">
	In IEEE Software</a>, 2002.  This paper: Required Reading.
      Download Splint source code and document
      from <a href="http://www.splint.org"> www. splint. org</a>,
      University of Virginia.</li>

    <li>Simson Garfinkel, Gene Spafford Practical Unix and Internet
    Security, O'Reilly &amp; Associates;
    Chapter 23: Writing Secure SUID and Network Programs. 
    Required Reading.</li>

    <li>Gerard J. Holzmann, The Power of Ten -- Rules for Developing
      Safety Critical Code,'' IEEE Computer, June 2006, pp. 93-95
      <a href="http://spinroot.com/gerard/pdf/P10.pdf">
	http://spinroot.com/gerard/pdf/P10.pdf</a>  Required Reading.</li>
	<li>Prabhaker Mateti, "Practical Advice on Writing Pre-
    Post-Conditions for Real Programs," Lecture Notes,  May
    1998. [<a href="PrePost/prepostNotesPM.html">local
    copy</a>]  Required Reading.</li>

    <li>Prabhaker Mateti, "Buffer Overflow", Lectures on Internet
    Security, <a href="http://www.cs.wright.edu/%7Epmateti/Courses/429/Top/lectures.html">
    www.cs.wright.edu /~pmateti/ Courses/ 429/ Top/ lectures.html</a>.
    There is a section on robust programming techniques that avoid the
    buffer overflow exploits.  Required Reading.</li>

    <li>Adam Shostack, "Security Code Review Guidelines," July
    2000, <a href="http://www.homeport.org/%7Eadam/review.html">
    www. homeport.org/ ~adam/ review.html</a> Highly Recommended
    Reading.</li>

    <li>David A. Wheeler, "Secure Programming ...", (i) <a href=
    "http://www.linuxdoc.org/HOWTO/Secure-Programs-HOWTO.html">
	Secure-Programs-HOWTO</a>, 2003. (ii)
      <a href="http://www.ibm.com/developerworks/linux/library/l-sprace/index.html">
	Prevent race conditions, 2004.</a>  Required Reading.</li>

  </ol>

<hr size="1">

<a href="../../../copyright.html">Copyright &copy;</a> 2012 &bull;
<a href="mailto:pmateti@wright.edu?subject=CEG429/InternetSecurity">
  pmateti@wright.edu</a> &bull;
<a href="http://s17.sitemeter.com/stats.asp?site=s17pmateti">
<img src="http://s17.sitemeter.com/meter.asp?site=s17pmateti"/></a> &bull;
<a href="../../lectures.html">Other Internet Security Lectures</a>

</body>
</html>
